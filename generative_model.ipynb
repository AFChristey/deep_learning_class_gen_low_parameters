{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1]\n",
    "Model architecture adapted from:\n",
    "Tim Pearce. Conditional Diffusion MNIST. https : / / github . com / TeaPearce /\n",
    "Conditional_Diffusion_MNIST/. Accessed: 22-01-2024.\n",
    "\n",
    "\n",
    "[2]\n",
    "@inproceedings{zhang2018perceptual,\n",
    "  title={The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},\n",
    "  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},\n",
    "  booktitle={CVPR},\n",
    "  year={2018}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as disp\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as fn\n",
    "\n",
    "from torchvision import transforms\n",
    "from einops import rearrange\n",
    "\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import einops\n",
    "\n",
    "from torchvision import transforms\n",
    "from einops import rearrange\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "assert torch.zeros(32).to(device).device.type=='cuda' # check cuda is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to make getting another batch of data easier\n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "                \n",
    "def normalise(x):\n",
    "    return (x-x.min())/(x.max()-x.min())\n",
    "\n",
    "class_names = ['apple','aquarium_fish','baby','bear','beaver','bed','bee','beetle','bicycle','bottle','bowl','boy','bridge','bus','butterfly','camel','can','castle','caterpillar','cattle','chair','chimpanzee','clock','cloud','cockroach','couch','crab','crocodile','cup','dinosaur','dolphin','elephant','flatfish','forest','fox','girl','hamster','house','kangaroo','computer_keyboard','lamp','lawn_mower','leopard','lion','lizard','lobster','man','maple_tree','motorcycle','mountain','mouse','mushroom','oak_tree','orange','orchid','otter','palm_tree','pear','pickup_truck','pine_tree','plain','plate','poppy','porcupine','possum','rabbit','raccoon','ray','road','rocket','rose','sea','seal','shark','shrew','skunk','skyscraper','snail','snake','spider','squirrel','streetcar','sunflower','sweet_pepper','table','tank','telephone','television','tiger','tractor','train','trout','tulip','turtle','wardrobe','whale','willow_tree','wolf','woman','worm',]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR100('data', train=True, download=True, transform=torchvision.transforms.Compose([\n",
    "        transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "#         torchvision.transforms.ToTensor()\n",
    "        \n",
    "    ])),\n",
    "    batch_size=124, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR100('data', train=False, download=True, transform=torchvision.transforms.Compose([\n",
    "#         torchvision.transforms.ToTensor()\n",
    "#         Do i ADD THIS???\n",
    "        transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])),\n",
    "    batch_size=124, drop_last=True)\n",
    "\n",
    "train_iterator = iter(cycle(train_loader))\n",
    "test_iterator = iter(cycle(test_loader))\n",
    "\n",
    "print(f'> Size of training dataset {len(train_loader.dataset)}')\n",
    "print(f'> Size of test dataset {len(test_loader.dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's view some of the training data\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "x,t = next(train_iterator)\n",
    "x,t = x.to(device), t.to(device)\n",
    "plt.imshow(torchvision.utils.make_grid(x*0.5+0.5).cpu().numpy().transpose(1, 2, 0), cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# From [1] https://github.com/TeaPearce/Conditional_Diffusion_MNIST/blob/main/script.py#L226\n",
    "def ddpm_schedules(beta1, beta2, T):\n",
    "    \"\"\"\n",
    "    Returns pre-computed schedules for DDPM sampling, training process.\n",
    "    \"\"\"\n",
    "    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
    "\n",
    "    beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32, device=device) / T + beta1\n",
    "\n",
    "    sqrt_beta_t = torch.sqrt(beta_t)\n",
    "    alpha_t = 1 - beta_t\n",
    "    log_alpha_t = torch.log(alpha_t)\n",
    "    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n",
    "\n",
    "    sqrtab = torch.sqrt(alphabar_t).view(-1,1,1,1)\n",
    "    oneover_sqrta = 1 / torch.sqrt(alpha_t)\n",
    "\n",
    "    sqrtmab = torch.sqrt(1 - alphabar_t).view(-1,1,1,1)\n",
    "    mab_over_sqrtmab_inv = (1 - alpha_t) / sqrtmab.squeeze()\n",
    "\n",
    "    return {\n",
    "        \"alpha_t\": alpha_t,  # \\alpha_t\n",
    "        \"oneover_sqrta\": oneover_sqrta,  # 1/\\sqrt{\\alpha_t}\n",
    "        \"sqrt_beta_t\": sqrt_beta_t,  # \\sqrt{\\beta_t}\n",
    "        \"alphabar_t\": alphabar_t,  # \\bar{\\alpha_t}\n",
    "        \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}}\n",
    "        \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}}\n",
    "        \"mab_over_sqrtmab\": mab_over_sqrtmab_inv,  # (1-\\alpha_t)/\\sqrt{1-\\bar{\\alpha_t}}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model architecture adapted from [1] https://github.com/TeaPearce/Conditional_Diffusion_MNIST/\n",
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, is_res: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.same_channels = in_channels==out_channels\n",
    "        self.is_res = is_res\n",
    "#       Convolution 1 \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "#       Convolution 2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.is_res:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            # this adds on correct residual in case channels have increased\n",
    "#             If number of output channels is same as input channels:\n",
    "            if self.same_channels:\n",
    "                out = x + x2\n",
    "            else:\n",
    "                out = x1 + x2 \n",
    "#             1.414 is normalization factor normally applied in residuals\n",
    "            return out / 1.414\n",
    "        else:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            return x2\n",
    "\n",
    "\n",
    "class UnetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetDown, self).__init__()\n",
    "#         process and downscale the image feature maps\n",
    "        layers = [ResidualConvBlock(in_channels, out_channels), nn.MaxPool2d(2)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetUp, self).__init__()\n",
    "#         process and upscale the image feature maps\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
    "#             ResidualConvBlock(out_channels, out_channels),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = torch.cat((x, skip), 1)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "# Used to embed context and time information.\n",
    "class EmbedFC(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim):\n",
    "        super(EmbedFC, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat = 256, n_classes=100):\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        self.down1 = UnetDown(n_feat, n_feat)\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)\n",
    "\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_classes, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_classes, 1*n_feat)\n",
    "\n",
    "        self.up0 = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(6 * n_feat, 2 * n_feat, 7, 7), # when concat temb and cemb end up w 6*n_feat\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, 8, 8), # otherwise just have 2*n_feat\n",
    "            nn.GroupNorm(8, 2 * n_feat),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),\n",
    "            nn.GroupNorm(8, n_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c, t):\n",
    "        # x is (noisy) image, c is context label, t is timestep, \n",
    "        # context_mask says which samples to block the context on\n",
    "        x = self.init_conv(x)\n",
    "        down1 = self.down1(x)\n",
    "        \n",
    "        down2 = self.down2(down1)\n",
    "    \n",
    "        hiddenvec = self.to_vec(down2)\n",
    "\n",
    "        # convert context to one hot embedding\n",
    "        c = nn.functional.one_hot(c, num_classes=100).type(torch.float).to(device)\n",
    "\n",
    "        # embed context, time step\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "                \n",
    "        up1 = self.up0(hiddenvec)\n",
    "        up2 = self.up1(cemb1*up1+temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2*up2+temb2, down1)  # add and multiply embeddings\n",
    "        \n",
    "#       Concat up3 with the input (x)\n",
    "        output = torch.cat((up3, x), 1)\n",
    "\n",
    "        out = self.out(output)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture adapted from [1] https://github.com/TeaPearce/Conditional_Diffusion_MNIST/\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(DDPM, self).__init__()\n",
    "#         self.net = UNet()\n",
    "        self.net = ContextUnet(in_channels=3, n_feat=48, n_classes=100)\n",
    "\n",
    "        # register_buffer allows accessing dictionary produced by ddpm_schedules\n",
    "        # e.g. can access self.sqrtab later\n",
    "        for k, v in ddpm_schedules(1e-4, 0.02, 400).items():\n",
    "            self.register_buffer(k, v)\n",
    "        \n",
    "        \n",
    "    # algorithm 1 in DDPM paper\n",
    "#     def forward(self, x):\n",
    "    def forward(self, x, c):\n",
    "#         Randomly samples time steps between 1 and T for each input in the batch.\n",
    "#         As you go from 1 to T (or opposite?) the strength of noise added to the image will increase\n",
    "        _ts = torch.randint(1, T+1, (x.shape[0],)).to(device)\n",
    "#         Generates random noise eps with the same shape as x - uses gaussian distribution - ϵ ∼ N (0, I)\n",
    "        eps = torch.randn_like(x) \n",
    "#         Partially diffuse x using the variance schedule - some images will be have large amounts of noise,\n",
    "#         while others might only have a bit - depends on variance schedule (T)\n",
    "        x_t = (self.sqrtab[_ts] * x + self.sqrtmab[_ts] * eps)\n",
    "        ts_ret = _ts/400\n",
    "    \n",
    "#         Input this partially diffused model into the generator. It is trying to predict the noise from the input.\n",
    "#         Calculate how close the model has predicted the noise from the input using MSE loss, \n",
    "#         comparing the generated noise to the actual noise added to the image.\n",
    "# Is z = self.net(x_t, c, ts_ret, context_mask)????????\n",
    "        return F.mse_loss(eps, self.net(x_t , c, ts_ret))\n",
    "    \n",
    "    # algorithm 2 in DDPM paper\n",
    "    def sample(self, n_sample, size, c):\n",
    "#         Initializes a random tensor x_i\n",
    "        x_i = torch.randn(n_sample, *size).to(device)\n",
    "\n",
    "#         High T = high noise, low T = low noise. Starts with high noise, then as for loop continues, noise decreases.\n",
    "        for i in range(T, 0, -1):\n",
    "#             Creates a tensor representing the time step in the current iteration.\n",
    "            t_is = torch.tensor([i / 400]).to(device)\n",
    "#             Repeats the time step tensor to match the size of the current batch\n",
    "            t_is = t_is.repeat(n_sample,1,1,1)\n",
    "#             When we reach the final time step, we dont want to add any random noise (z) so we set to 0\n",
    "            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0\n",
    "#             This tries to predict the nosie from the input\n",
    "            eps = self.net(x_i, c, t_is).to(device)\n",
    "    \n",
    "#             This subtracts the predicted noise from the generated model to update the generated model.\n",
    "#             It also adds some noise to the model (z) that has been scaled according to the schedule \n",
    "#             (to ensure that the generated samples follow a distribution that matches the training data).\n",
    "            x_i = (self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i]) + self.sqrt_beta_t[i] * z ).to(device)\n",
    "        return x_i\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm = DDPM().to(device)\n",
    "step = 0\n",
    "loss_smooth = 2.00\n",
    "# Adam optimiser\n",
    "optimiser = torch.optim.Adam(ddpm.parameters(), lr=0.001)\n",
    "T = 400\n",
    "\n",
    "print(f'> Number of model parameters {len(torch.nn.utils.parameters_to_vector(ddpm.parameters()))}')\n",
    "if len(torch.nn.utils.parameters_to_vector(ddpm.parameters())) > 1000000:\n",
    "    print(\"> Warning: you have gone over your parameter budget and will have a grade penalty!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision.utils as vutils\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Creates a learning rate scheduler using StepLR, step size 1000\n",
    "scheduler = StepLR(optimiser, step_size=1000, gamma=0.97)\n",
    "\n",
    "    \n",
    "while step < 50000:\n",
    "#   Put model in training mode\n",
    "    ddpm.train()\n",
    "\n",
    "#   Obtain next batch of data with labels\n",
    "    x, c = next(train_iterator)\n",
    "    x, c = x.to(device), c.to(device)\n",
    "\n",
    "#   Calculate loss between generated noise and actual noise\n",
    "    loss = ddpm(x, c) # modified with conditionals\n",
    "#   Clear gradients of all obtimized tensors\n",
    "    optimiser.zero_grad()\n",
    "#     Use this loss to update the weights of the model.\n",
    "#   Compute gradient of loss with respect to model parameters\n",
    "    loss.backward()\n",
    "#   Update model parameters using computed gradients\n",
    "    optimiser.step()\n",
    "#   Adjusts the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "#     Updates a smoothed version of the loss for display purposes\n",
    "    loss_smooth = loss_smooth + 0.01 * (loss.item() - loss_smooth)\n",
    "    \n",
    "    if step % 1000 == 0:\n",
    "#       Put into evaluation mode\n",
    "        ddpm.eval()\n",
    "        print(f'step: {step:4d} train loss: {loss_smooth:.3f}')\n",
    "            \n",
    "        with torch.no_grad():\n",
    "#           Limits the size of the condition c to the first 64 samples, since batch size is 300+\n",
    "            c = c[:64]\n",
    "            \n",
    "#           Sample from the model using a batch of gaussian noise.\n",
    "            samples_batch = ddpm.sample(64, (3, 32, 32), c).to(x.device)\n",
    "            \n",
    "    \n",
    "#           Limits the size of the input data x to the first 64 samples\n",
    "            x = x[:64]\n",
    "            img_grid = rearrange(\n",
    "                torch.cat([(samples_batch * 0.5 + 0.5), (x * 0.5 + 0.5)], dim=0),\n",
    "                '(b1 b2) c h w -> (b1 h) (b2 w) c',\n",
    "                b1=2\n",
    "            )\n",
    "            plt.figure(figsize=(100, 100))  # Set the figure size\n",
    "            plt.imshow((img_grid.cpu() * 255).int().numpy())\n",
    "            plt.show()\n",
    "\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model.\n",
    "# model_path = 'Models/generative.pth'\n",
    "# torch.save(ddpm, model_path)\n",
    "\n",
    "\n",
    "# Load model.\n",
    "model_path = 'Models/generative.pth'\n",
    "ddpm = torch.load(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For viewing interpolations between pairs of samples. \n",
    "# NOTE: Ensure that the second (commented out) ddpm is being used instead of the first dppm.\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "ddpm.eval()\n",
    "\n",
    "# Generate a grid for subplots\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "x, c = next(train_iterator)\n",
    "x, c = x.to(device), c.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    c = torch.randint(0, 100, (8,))\n",
    "    size = (3, 32, 32)\n",
    "    n_sample = 8\n",
    "\n",
    "    x_i = torch.randn(n_sample, *size).to(device)\n",
    "\n",
    "    samples = ddpm.sample(n_sample, size, c, x_i).to(device).detach()\n",
    "\n",
    "# Create a grid of images\n",
    "grid = torchvision.utils.make_grid(samples * 0.5 + 0.5, nrow=10, padding=2, normalize=True)\n",
    "\n",
    "# Convert the grid to a NumPy array and transpose channels\n",
    "grid_np = grid.cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "# Display the grid\n",
    "plt.imshow(grid_np)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install clean-fid\n",
    "import os\n",
    "from cleanfid import fid\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define directories\n",
    "real_images_dir = 'real_images'\n",
    "generated_images_dir = 'generated_images'\n",
    "num_samples = 10000 # do not change\n",
    "\n",
    "ddpm.eval()\n",
    "\n",
    "\n",
    "# create/clean the directories\n",
    "def setup_directory(directory):\n",
    "    if os.path.exists(directory):\n",
    "        !rm -r {directory} # remove any existing (old) data\n",
    "    os.makedirs(directory)\n",
    "\n",
    "setup_directory(real_images_dir)\n",
    "setup_directory(generated_images_dir)\n",
    "\n",
    "# generate and save 10k model samples\n",
    "num_generated = 0\n",
    "while num_generated < num_samples:\n",
    "    print(num_generated)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        c = torch.randint(0, 100, (64,))\n",
    "\n",
    "        samples_batch = ddpm.sample(64, (3, 32, 32), c).to(device).detach()\n",
    "\n",
    "        for image in samples_batch:\n",
    "            if num_generated >= num_samples:\n",
    "                break\n",
    "            save_image(image, os.path.join(generated_images_dir, f\"gen_img_{num_generated}.png\"))\n",
    "            num_generated += 1\n",
    "\n",
    "# save 10k images from the CIFAR-100 test dataset\n",
    "num_saved_real = 0\n",
    "while num_saved_real < num_samples:\n",
    "    real_samples_batch, _ = next(test_iterator)\n",
    "    for image in real_samples_batch:\n",
    "        if num_saved_real >= num_samples:\n",
    "            break\n",
    "        save_image(image, os.path.join(real_images_dir, f\"real_img_{num_saved_real}.png\"))\n",
    "        num_saved_real += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute FID\n",
    "\n",
    "real_images_dir = 'real_images'\n",
    "generated_images_dir = 'generated_images'\n",
    "score = fid.compute_fid(real_images_dir, generated_images_dir, mode=\"clean\")\n",
    "print(f\"FID score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View nearest neighbours using LPIPS [2]: (reference at top)\n",
    "import lpips\n",
    "\n",
    "ddpm.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "#     Random batch\n",
    "#     context1 = torch.randint(0, 100, (64,)).to(device)\n",
    "#     One type of image batch - in this case label 88\n",
    "    context1 = torch.full((64,), 88, dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "    # Sample images from the diffusion model\n",
    "    sample1 = ddpm.sample(64, (3, 32, 32), context1).to(device).detach()\n",
    "    sample1 = sample1 * 0.5 + 0.5\n",
    "\n",
    "# Create an LPIPS model\n",
    "loss_fn = lpips.LPIPS(net='alex').to(device)\n",
    "\n",
    "\n",
    "# Function to calculate LPIPS distance between two images\n",
    "def lpips_distance(img1, img2):\n",
    "    with torch.no_grad():\n",
    "        distance = loss_fn(img1.unsqueeze(0), img2.unsqueeze(0)).item()\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "# Function to find nearest neighbors\n",
    "def find_nearest_neighbors(target_image, image_list, k=6):\n",
    "#   Computes a list of distances between the target_image and each image in image_list\n",
    "    distances = [lpips_distance(target_image, img) for img in image_list]\n",
    "#   Creates a list of indices corresponding to the sorted order of distances\n",
    "    indices = sorted(range(len(distances)), key=lambda i: distances[i])\n",
    "#   Returns a list of the k nearest neighbor images from image_list\n",
    "    return [image_list[i] for i in indices[:k]], indices[:k]\n",
    "\n",
    "# Select a target image for finding neighbors\n",
    "target_index = 0  # Choose the index of the target image\n",
    "target_image = sample1[target_index].to(device)\n",
    "\n",
    "# Find nearest neighbors\n",
    "nearest_neighbors, indices = find_nearest_neighbors(target_image, sample1)\n",
    "\n",
    "# Display the target image and its nearest neighbors\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, len(nearest_neighbors)-2, 1)\n",
    "plt.imshow(target_image.cpu().numpy().transpose(1, 2, 0))\n",
    "plt.title('Target Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Plot some of the nearest neighbors.\n",
    "for i, neighbor_img in enumerate(nearest_neighbors):\n",
    "#   Ignore the first image since it is the target_image \n",
    "    if i == 0:\n",
    "        print('/')\n",
    "    else:\n",
    "        plt.subplot(1, len(nearest_neighbors) + 1, i + 2)\n",
    "        plt.imshow(neighbor_img.cpu().numpy().transpose(1, 2, 0))\n",
    "        plt.title(f'Neighbor {i}')\n",
    "        plt.axis('off')\n",
    "\n",
    "\n",
    "# plt.savefig('LPIPSSameTIGER.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples:\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "ddpm.eval()\n",
    "\n",
    "# Generate samples\n",
    "with torch.no_grad():\n",
    "    c = torch.randint(0, 100, (64,))\n",
    "    samples_batch = ddpm.sample(64, (3, 32, 32), c).to(device)\n",
    "\n",
    "# Plot and save the figure\n",
    "plt.imshow(torchvision.utils.make_grid(samples_batch * 0.5 + 0.5).cpu().numpy().transpose(1, 2, 0), cmap=plt.cm.binary)\n",
    "plt.axis(\"off\")  # Turn off axis labels\n",
    "plt.title(\"Generated Samples\")\n",
    "plt.savefig('generated_samples.png')  # Save the figure as an image\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# To show one batch of 5 images with the same label.\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "\n",
    "ddpm.eval()\n",
    "\n",
    "# Define label number\n",
    "label_numbers = [99]\n",
    "\n",
    "for label_number in label_numbers:\n",
    "    print(class_names[label_number])\n",
    "\n",
    "    # Generate samples for the given label\n",
    "    with torch.no_grad():\n",
    "        c = torch.full((5,), label_number)\n",
    "        samples_batch = ddpm.sample(5, (3, 32, 32), c).to(device)\n",
    "\n",
    "    # Plot the label and the generated samples\n",
    "    plt.text(2, label_number - 0.5, f\"Label: {label_number}\", color='red', fontweight='bold', ha='center', va='center')\n",
    "\n",
    "    for i in range(5):\n",
    "        plt.subplot(len(label_numbers), 5, len(label_numbers) * i + label_numbers.index(label_number) + 1)\n",
    "        plt.imshow((samples_batch[i] * 0.5 + 0.5).cpu().numpy().transpose(1, 2, 0), cmap=plt.cm.binary)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "# Save the figure as an image\n",
    "# plt.savefig('generated_samples_for_labels.png')\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate a sample of 5 specific images based off of specific labels:\n",
    "# To form cherry-picked images and bad batch of images\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "ddpm.eval()\n",
    "\n",
    "# Generate samples\n",
    "with torch.no_grad():\n",
    "#     c = torch.randint(0, 100, (64,))\n",
    "    c = [2,11,35,46,98]\n",
    "#     c = [88]\n",
    "    c = torch.tensor(c)\n",
    "\n",
    "    samples_batch = ddpm.sample(5, (3, 32, 32), c).to(device)\n",
    "\n",
    "# Plot and save the figure\n",
    "plt.imshow(torchvision.utils.make_grid(samples_batch * 0.5 + 0.5).cpu().numpy().transpose(1, 2, 0), cmap=plt.cm.binary)\n",
    "plt.axis(\"off\")  # Turn off axis labels\n",
    "# plt.title(\"Generated Samples\")\n",
    "plt.savefig('bad_people_samples.png')  # Save the figure as an image\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n",
    "\n",
    "for i in c:\n",
    "    print(class_names[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
